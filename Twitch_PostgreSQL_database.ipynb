{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitch PostgreSQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PostgreSQL connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://naysan.ca/2020/05/31/postgresql-to-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Hugo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librairies\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters to login\n",
    "co_param = {\n",
    "    \"host\"      : \"twitch.caampywfg0rz.us-east-1.rds.amazonaws.com\",\n",
    "    \"database\"  : \"Twitch\",\n",
    "    \"user\"      : \"GaTech_team_96\",\n",
    "    \"password\"  : \"i-love-my-coffee-without-milk-and-sugar-at-800AM\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(co_param):\n",
    "    \"\"\"\n",
    "    Connect to the PostgreSQL database server\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**co_param)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        sys.exit(1) \n",
    "    print(\"Connection successful\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgresql_to_dataframe(conn, select_query, column_names):\n",
    "    \"\"\"\n",
    "    Tranform a SELECT query into a pandas dataframe\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(select_query)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    \n",
    "    # Naturally we get a list of tupples\n",
    "    tupples = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    \n",
    "    # We just need to turn it into a pandas dataframe\n",
    "    df = pd.DataFrame(tupples, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream_data database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "# SQL query\n",
    "sql_query = \"\"\"SELECT * FROM stream_data\"\"\"\n",
    "\n",
    "# Column names\n",
    "stream_data_col_names = [\"game_id\",\"stream_id\",\"language\",\"started_at\",\"title\",\n",
    "                            \"stream_type\",\"user_id\",\"user_name\",\"viewer_count\",\"user_login\",\"game_name\",\n",
    "                            \"thumbnail_url\",\"tag_ids\",\"is_mature\",\"time_logged\"]\n",
    "\n",
    "# Retrieving the data\n",
    "stream_data = postgresql_to_dataframe(connect(co_param), sql_query, stream_data_col_names)\n",
    "# stream_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2591336, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing formatting from \n",
    "stream_data.loc[:,\"started_at\"] = stream_data.loc[:,\"started_at\"].map(lambda x: x.rstrip(\"Z\"))\n",
    "stream_data.loc[:,\"started_at\"] = stream_data.loc[:,\"started_at\"].map(lambda x: x.replace(\"T\", \" \"))\n",
    "stream_data[\"stream_duration_hours\"] = pd.to_datetime(stream_data[\"time_logged\"])-pd.to_datetime(stream_data[\"started_at\"])\n",
    "stream_data['stream_duration_hours'] = stream_data['stream_duration_hours']/np.timedelta64(1, 'h')\n",
    "\n",
    "\n",
    "# Changing is_mature with True =1 & False = 0\n",
    "stream_data.loc[stream_data[\"is_mature\"] == True, \"is_mature\"] = 1\n",
    "stream_data.loc[stream_data[\"is_mature\"] == False, \"is_mature\"] = 0\n",
    "\n",
    "stream_data.head()\n",
    "\n",
    "# stream_data.to_csv('twitch_stream_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. filtering at english game\n",
    "1. temp => get the diff of time_logged and started_at\n",
    "2. how is the game behaving after 1week or 2? >>> defining a target for the MLalgo\n",
    "        input target\n",
    "        avg/medium/total viewers >>> \n",
    "3. Post-temp >>> sentiment analysis >>> title\n",
    "4. Audience target > is_mature of the game\n",
    "5. python dict for the categories of the game\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream_data_ENG database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a temp database\n",
    "df_temp = stream_data\n",
    "\n",
    "# Selecting only English stream\n",
    "df_temp = df_temp[df_temp[\"language\"] == \"en\"]\n",
    "\n",
    "# Computing the final table\n",
    "stream_data_ENG = df_temp\n",
    "stream_data_ENG.head()\n",
    "\n",
    "# stream_data_ENG.to_csv('twitch_data_stream_data_ENG.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_game_avg_maturity database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a temp database\n",
    "df = stream_data_ENG[[\"game_id\", \"time_logged\", \"is_mature\"]].copy()\n",
    "\n",
    "# changing the format of the col\n",
    "df[\"game_id\"] = df[\"game_id\"].astype('str')\n",
    "df[\"avg_maturity\"] = df[\"is_mature\"].astype('float')\n",
    "\n",
    "# Grouping by \"game_id\" and \"time_logged\" and its avg \"is_mature\"\n",
    "df_game_avg_maturity = df.groupby([\"game_id\", \"time_logged\"]).mean()\n",
    "df_game_avg_maturity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitch Tags and their categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "# twitch_tags_cat = pd.read_csv('api_connection\\Twitch_tags.csv')\n",
    "\n",
    "# Linux/Mac\n",
    "twitch_tags_cat = pd.read_csv('./api_connection/Twitch_tags.csv')\n",
    "\n",
    "twitch_tags_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitch_tags_cat[['TagId', 'TagName']].to_dict()\n",
    "twitch_tags_cat[twitch_tags_cat['TagId'] == '6ea6bca4-4712-4ab9-a906-e3336a9d8039']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding time of day into 6 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_STREAM_TIME_THRESHOLD = 0.5 # Hours\n",
    "MAX_STREAM_TIME_THRESHOLD = 7 # Hours\n",
    "\n",
    "df_with_encoded_time = stream_data_ENG\n",
    "df_with_encoded_time['log_date'] = pd.to_datetime(df_with_encoded_time['time_logged']).dt.date\n",
    "df_with_encoded_time['time_logged_encoded'] = pd.to_datetime(df_with_encoded_time['time_logged']).dt.hour\n",
    "df_with_encoded_time['time_logged_encoded'] =  df_with_encoded_time['time_logged_encoded']//4\n",
    "\n",
    "df_with_encoded_time = df_with_encoded_time[\n",
    "    (df_with_encoded_time['stream_duration_hours'] > MIN_STREAM_TIME_THRESHOLD) & \n",
    "    (df_with_encoded_time['stream_duration_hours'] < MAX_STREAM_TIME_THRESHOLD)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "si = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_row(row):\n",
    "    sentiment = si.polarity_scores(row)\n",
    "    return pd.Series([sentiment['pos'], sentiment['neg'], sentiment['neu']])\n",
    "\n",
    "df_with_encoded_time['positive_sentiment'] = 0.0\n",
    "df_with_encoded_time['negative_sentiment'] = 0.0\n",
    "df_with_encoded_time['neutral_sentiment'] = 0.0\n",
    "\n",
    "df_with_encoded_time[['positive_sentiment',\n",
    "                     'negative_sentiment',\n",
    "                     'neutral_sentiment']] =  df_with_encoded_time.title.apply(lambda row: get_sentiment_row(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mean, median, total viewership \n",
    "df_with_features = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['viewer_count']].mean().reset_index()\n",
    "df_with_features = df_with_features.rename(columns={'viewer_count': 'mean_viewer_count'})\n",
    "\n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['viewer_count']].median().reset_index()\n",
    "temp_df = temp_df.rename(columns={'viewer_count': 'median_viewer_count'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['viewer_count']].sum().reset_index()\n",
    "temp_df = temp_df.rename(columns={'viewer_count': 'total_viewer_count'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "\n",
    "# Add mean, median, total stream time \n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['stream_duration_hours']].mean().reset_index()\n",
    "temp_df = temp_df.rename(columns={'stream_duration_hours': 'mean_stream_duration_hours'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['stream_duration_hours']].median().reset_index()\n",
    "temp_df = temp_df.rename(columns={'stream_duration_hours': 'median_stream_duration_hours'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['stream_duration_hours']].sum().reset_index()\n",
    "temp_df = temp_df.rename(columns={'stream_duration_hours': 'total_stream_duration_hours'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "\n",
    "# Add average matrure rating \n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['is_mature']].mean().reset_index()\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "\n",
    "# Add mean sentiment\n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['positive_sentiment']].mean().reset_index()\n",
    "temp_df = temp_df.rename(columns={'positive_sentiment': 'mean_positive_sentiment'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['negative_sentiment']].mean().reset_index()\n",
    "temp_df = temp_df.rename(columns={'negative_sentiment': 'mean_negative_sentiment'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "temp_df = df_with_encoded_time.groupby(['time_logged_encoded', 'game_name', 'log_date'])[['neutral_sentiment']].mean().reset_index()\n",
    "temp_df = temp_df.rename(columns={'neutral_sentiment': 'mean_neutral_sentiment'})\n",
    "df_with_features = pd.merge(df_with_features, temp_df)\n",
    "\n",
    "df_with_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features.to_csv('twitch_data_processed_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feauture_column_names = list(df_with_features.columns)[3:-3]\n",
    "feauture_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_duration_days = 3\n",
    "\n",
    "training_data_frame_dictionary = {\n",
    "    \n",
    "}\n",
    "for time_slot in df_with_features.time_logged_encoded.unique():\n",
    "    df_filtered_on_timeslot = df_with_features[df_with_features['time_logged_encoded'] == time_slot].copy()\n",
    "    \n",
    "    training_data_frame_dictionary[time_slot] = {\n",
    "        \n",
    "    }\n",
    "    \n",
    "    for game in df_filtered_on_timeslot.game_name.unique():\n",
    "        df_filtered_on_game = df_filtered_on_timeslot[df_filtered_on_timeslot['game_name'] == game].copy()\n",
    "        df_filtered_on_game = df_filtered_on_game.sort_values(by='log_date').reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        for col in feauture_column_names:\n",
    "            df_filtered_on_game['target_'+ col + '_' + str(shift_duration_days)] = df_filtered_on_game[col].shift(-shift_duration_days)\n",
    "        \n",
    "        training_data_frame_dictionary[time_slot][game] = df_filtered_on_game.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dictionary with following hierarcy\n",
    "\n",
    "-time slot\n",
    "    - game name\n",
    "        - DataFrame with targets \n",
    "\n",
    "\"\"\" \n",
    "training_data_frame_dictionary[0]['7 Days to Die']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('saved_dictionary.pkl', 'wb') as f:\n",
    "    pickle.dump(training_data_frame_dictionary, f)\n",
    "        \n",
    "# with open('saved_dictionary.pkl', 'rb') as f:\n",
    "#     loaded_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### game_info database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query\n",
    "sql_query = \"\"\"SELECT * FROM game_info\"\"\"\n",
    "\n",
    "# Column names\n",
    "game_info_col_names = [\"game_id\",\"game_name\",\"game_picture_url\",\"time_logged\"]\n",
    "\n",
    "# Retrieving the data\n",
    "game_info = postgresql_to_dataframe(connect(co_param), sql_query, game_info_col_names)\n",
    "game_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_data_ENG\n",
    "# ast.literal_eval(stream_data_ENG.tag_ids[0])\n",
    "import re\n",
    "h = re.compile(\"['\\{][0-9A-Za-z\\-]+[,\\}]\")\n",
    "\n",
    "h.match(stream_data_ENG.tag_ids[50])[0][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3060cf370e9fd446e256e0e075484898b983bef6e1d272fbaceccb953fde7acc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
